#!/bin/sh

version="0.0.1"
walldir="/tmp/wallhaven"
# number of pages to show in search results
# each page contains 24 results
max_pages=2
# sorting : date_added, relevance, random, views, favorites, toplist
sorting=relevance
# quality : large original small
quality=large
# atleast : least res
atleast=1920x1080

# allow the user to customize the defaults
[ -e "$HOME/.config/waldlrc" ] && . "$HOME/.config/waldlrc"

sh_menu () {
	rofi -dmenu -l 0 -p "search wallpapers"
}

##########################
## getting search query ##
##########################

[ -n "$*" ] && query="$*" || query=$( sh_menu )
[ -z "$query" ] && exit 1
query=$(printf '%s' "$query" | tr ' ' '+' )

######################
## start up commands #
######################

rm -rf "$walldir"
mkdir -p "$walldir" 

# progress display command
sh_info () {
	printf "%s\n" "$1" >&2
	notify-send "wallhaven" "$1"
	[ -n "$2" ] && exit "$2"
}

# dependency checking
dep_ck () {
	for pr; do
		command -v $pr >/dev/null 2>&1 || sh_info "command $pr not found, install: $pr" 1
	done
}
dep_ck "imv" "curl" "jq" "wget"


# clean up command that would be called when the program exits
clean_up () {
	printf "%s\n" "cleaning up..." >&2
	rm -rf "$datafile"
}

# data file to store the api information
datafile="/tmp/wald.$$"

# clean up if killed
trap "exit" INT TERM
trap "clean_up" EXIT

##################
## getting data ##
##################

# request the search results for each page
get_results () {
	for page_no in $(seq $max_pages)
	do
		{
			json=$(curl -s -G "https://wallhaven.cc/api/v1/search" \
					-d "q=$1" \
					-d "page=$page_no" \
					-d "atleast=$atleast" \
					-d "sorting=$sorting"
				)
			printf "%s\n" "$json" >> "$datafile"
		} &
		sleep 0.001
	done
	wait
}

# search wallpapers
sh_info "getting data..."
get_results "$query"

# check if data file is empty, if so then exit
[ -s "$datafile" ] || sh_info "no images found" 1 

thumbnails=$( jq -r '.data[]?|.thumbs.'"$quality" < "$datafile")

# extract the id's out of the thumbnail name
image_ids=$(echo $thumbnails)
[ -z "$image_ids" ] && exit

#########################
## download wallpapers ##
#########################

# download the selected wall papers
cd "$walldir"
sh_info "downloading wallpapers..."
for ids in $image_ids
do
	ids="${ids##*/}"
	ids="${ids%.*}"
	url=$( jq -r '.data[]?|select( .id == "'$ids'" )|.path' < "$datafile" )
	wget $url &
done
wait

sh_info "wallpapers downloaded in:- '$walldir'"
